{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Toxic Comment Classification Challenge\n","## BERT - TensorFlow 2 & Hugging Face Transformers Library"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:41:09.612330Z","iopub.status.busy":"2024-05-06T05:41:09.611973Z","iopub.status.idle":"2024-05-06T05:41:20.646926Z","shell.execute_reply":"2024-05-06T05:41:20.645256Z","shell.execute_reply.started":"2024-05-06T05:41:09.612262Z"},"id":"SuY02mplBrDy","outputId":"7812630a-8a77-4cdf-b99b-892778ca3898","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n","\u001b[K     |████████████████████████████████| 450kB 1.2MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (2.22.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (1.17.4)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (4.39.0)\n","Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (1.10.29)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (2019.11.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/78/fef8d089db5b97546fd6d1ff2e813b8544e85670bf3a8c378c9d0250b98d/sacremoses-0.0.53.tar.gz (880kB)\n","\u001b[K     |████████████████████████████████| 880kB 32.1MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0) (0.1.83)\n","Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (2019.9.11)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (1.24.2)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0) (3.0.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (0.9.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.29 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0) (1.13.29)\n","Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (1.13.0)\n","Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (7.0)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0) (0.14.0)\n","Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.3.0) (2.8.0)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.3.0) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-cp36-none-any.whl size=895254 sha256=f317636395cc07e6504330aec27025040e278bf0444fcdbf32870375eafd663c\n","  Stored in directory: /root/.cache/pip/wheels/56/d5/b2/bc878b2bbddfbcc8fd62ca73c4fd842bd28c1fd3dbdf424c74\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 transformers-2.3.0\n"]}],"source":["!pip install transformers==2.3.0"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:40:38.866860Z","iopub.status.busy":"2024-05-06T05:40:38.866560Z","iopub.status.idle":"2024-05-06T05:40:45.092014Z","shell.execute_reply":"2024-05-06T05:40:45.091376Z","shell.execute_reply.started":"2024-05-06T05:40:38.866814Z"},"id":"aGa2jmNmCjXf","trusted":true},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Data Pipeline\n","- Loading the datasets from CSVs\n","- Preprocessing (Tokenization, Truncation & Padding)\n","- Creating efficient data pipelines using tf.data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:40:55.174390Z","iopub.status.busy":"2024-05-06T05:40:55.174036Z","iopub.status.idle":"2024-05-06T05:40:55.178540Z","shell.execute_reply":"2024-05-06T05:40:55.177705Z","shell.execute_reply.started":"2024-05-06T05:40:55.174328Z"},"id":"5FuPBMEX_Lxv","trusted":true},"outputs":[],"source":["train_path = 'data/train.csv'\n","test_path = 'data/test.csv'\n","test_labels_path = 'data/test_labels.csv'\n","subm_path = 'data/sample_submission.csv'"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:40:56.539814Z","iopub.status.busy":"2024-05-06T05:40:56.539344Z","iopub.status.idle":"2024-05-06T05:40:58.186554Z","shell.execute_reply":"2024-05-06T05:40:58.185689Z","shell.execute_reply.started":"2024-05-06T05:40:56.539616Z"},"id":"50uZZZzH_NDV","outputId":"f925ad52-96d5-47a1-b573-db6fa12012ba","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","df_train = pd.read_csv(train_path)\n","df_test = pd.read_csv(test_path)\n","df_test_labels = pd.read_csv(test_labels_path)\n","df_test_labels = df_test_labels.set_index('id')\n","\n","df_train.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:42:02.419070Z","iopub.status.busy":"2024-05-06T05:42:02.418762Z","iopub.status.idle":"2024-05-06T05:51:00.549705Z","shell.execute_reply":"2024-05-06T05:51:00.549037Z","shell.execute_reply.started":"2024-05-06T05:42:02.419025Z"},"id":"g-8WnPfD_35q","outputId":"ef31e72c-9dc5-4e7d-875e-875758466538","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a18b9f17fd234f4296f1eba74665a063","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=159571), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["from transformers import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","bert_model_name = 'bert-base-uncased'\n","\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n","MAX_LEN = 128\n","\n","def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n","    tokenized_sentences = []\n","\n","    for sentence in tqdm(sentences):\n","        tokenized_sentence = tokenizer.encode(\n","                            sentence,                  # Sentence to encode.\n","                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                            max_length = max_seq_len,  # Truncate all sentences.\n","                    )\n","        \n","        tokenized_sentences.append(tokenized_sentence)\n","\n","    return tokenized_sentences\n","\n","def create_attention_masks(tokenized_and_padded_sentences):\n","    attention_masks = []\n","\n","    for sentence in tokenized_and_padded_sentences:\n","        att_mask = [int(token_id > 0) for token_id in sentence]\n","        attention_masks.append(att_mask)\n","\n","    return np.asarray(attention_masks)\n","\n","input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","attention_masks = create_attention_masks(input_ids)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:53:02.522659Z","iopub.status.busy":"2024-05-06T05:53:02.522326Z","iopub.status.idle":"2024-05-06T05:53:02.717646Z","shell.execute_reply":"2024-05-06T05:53:02.716962Z","shell.execute_reply.started":"2024-05-06T05:53:02.522611Z"},"id":"bMumeio9FFds","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","labels =  df_train[label_cols].values\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n","\n","train_size = len(train_inputs)\n","validation_size = len(validation_inputs)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:53:06.468979Z","iopub.status.busy":"2024-05-06T05:53:06.468675Z","iopub.status.idle":"2024-05-06T05:53:20.267547Z","shell.execute_reply":"2024-05-06T05:53:20.266816Z","shell.execute_reply.started":"2024-05-06T05:53:06.468935Z"},"id":"H99t9YNDFcRq","trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","NR_EPOCHS = 1\n","\n","def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n","    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n","    if train:\n","        dataset = dataset.shuffle(buffer_size=buffer_size)\n","    dataset = dataset.repeat(epochs)\n","    dataset = dataset.batch(batch_size)\n","    if train:\n","        dataset = dataset.prefetch(1)\n","    \n","    return dataset\n","\n","train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n","validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. BERT Model\n","- Load the pretrained BERT base-model from Transformers library\n","- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:53:34.870059Z","iopub.status.busy":"2024-05-06T05:53:34.869767Z","iopub.status.idle":"2024-05-06T05:53:49.937019Z","shell.execute_reply":"2024-05-06T05:53:49.936367Z","shell.execute_reply.started":"2024-05-06T05:53:34.870014Z"},"id":"3Zp6X7meF_T5","trusted":true},"outputs":[],"source":["from transformers import TFBertModel\n","from tensorflow.keras.layers import Dense, Flatten\n","\n","class BertClassifier(tf.keras.Model):    \n","    def __init__(self, bert: TFBertModel, num_classes: int):\n","        super().__init__()\n","        self.bert = bert\n","        self.classifier = Dense(num_classes, activation='sigmoid')\n","        \n","    @tf.function\n","    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n","        outputs = self.bert(input_ids,\n","                               attention_mask=attention_mask,\n","                               token_type_ids=token_type_ids,\n","                               position_ids=position_ids,\n","                               head_mask=head_mask)\n","        cls_output = outputs[1]\n","        cls_output = self.classifier(cls_output)\n","                \n","        return cls_output\n","\n","model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Training Loop\n","- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n","- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n","- AUC evaluation metrics"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T05:55:03.941889Z","iopub.status.busy":"2024-05-06T05:55:03.941581Z","iopub.status.idle":"2024-05-06T06:54:57.036088Z","shell.execute_reply":"2024-05-06T06:54:57.035347Z","shell.execute_reply.started":"2024-05-06T05:55:03.941843Z"},"id":"cJsVaX4qqoM0","outputId":"52dfd86b-a8f4-48f5-c481-a9db4f5c40fa","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================== EPOCH 0 ==================================================\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d35979612d3a4262a3291ebd7cdc00a5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=4487), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Train Step: 0, Loss: 0.9592233300209045\n","toxic roc_auc 0.450892835855484\n","severe_toxic roc_auc 0.0\n","obscene roc_auc 0.4166666865348816\n","threat roc_auc 0.0\n","insult roc_auc 0.39080461859703064\n","identity_hate roc_auc 0.0\n","\n","Train Step: 1000, Loss: 0.15134145319461823\n","toxic roc_auc 0.9014577269554138\n","severe_toxic roc_auc 0.8869020342826843\n","obscene roc_auc 0.9194873571395874\n","threat roc_auc 0.7024897933006287\n","insult roc_auc 0.8973801136016846\n","identity_hate roc_auc 0.80324786901474\n","\n","Train Step: 2000, Loss: 0.09692341089248657\n","toxic roc_auc 0.9811915755271912\n","severe_toxic roc_auc 0.986565351486206\n","obscene roc_auc 0.9887411594390869\n","threat roc_auc 0.9501869082450867\n","insult roc_auc 0.9843432307243347\n","identity_hate roc_auc 0.9693376421928406\n","\n","Train Step: 3000, Loss: 0.07791095227003098\n","toxic roc_auc 0.9847622513771057\n","severe_toxic roc_auc 0.9843725562095642\n","obscene roc_auc 0.9882120490074158\n","threat roc_auc 0.9830675721168518\n","insult roc_auc 0.9845612645149231\n","identity_hate roc_auc 0.9797573685646057\n","\n","Train Step: 4000, Loss: 0.06769558042287827\n","toxic roc_auc 0.9855169653892517\n","severe_toxic roc_auc 0.990494966506958\n","obscene roc_auc 0.9912533164024353\n","threat roc_auc 0.9798727631568909\n","insult roc_auc 0.9887965321540833\n","identity_hate roc_auc 0.9867024421691895\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8293f3fc2d00464baff3e8728012a397","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=498), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","Epoch 1, Validation Loss: 0.03734875097870827, Time: 3592.954366207123\n","\n","toxic roc_auc 0.9875776171684265\n","severe_toxic roc_auc 0.9882749319076538\n","obscene roc_auc 0.9912504553794861\n","threat roc_auc 0.9825899600982666\n","insult roc_auc 0.9858059287071228\n","identity_hate roc_auc 0.9792766571044922\n","\n","\n"]}],"source":["import time\n","from transformers import create_optimizer\n","\n","steps_per_epoch = train_size // BATCH_SIZE\n","validation_steps = validation_size // BATCH_SIZE\n","\n","# | Loss Function\n","loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","validation_loss = tf.keras.metrics.Mean(name='test_loss')\n","\n","# | Optimizer (with 1-cycle-policy)\n","warmup_steps = steps_per_epoch // 3\n","total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n","optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n","\n","# | Metrics\n","train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n","validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n","\n","@tf.function\n","def train_step(model, token_ids, masks, labels):\n","    labels = tf.dtypes.cast(labels, tf.float32)\n","\n","    with tf.GradientTape() as tape:\n","        predictions = model(token_ids, attention_mask=masks)\n","        loss = loss_object(labels, predictions)\n","\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n","\n","    train_loss(loss)\n","\n","    for i, auc in enumerate(train_auc_metrics):\n","        auc.update_state(labels[:,i], predictions[:,i])\n","        \n","@tf.function\n","def validation_step(model, token_ids, masks, labels):\n","    labels = tf.dtypes.cast(labels, tf.float32)\n","\n","    predictions = model(token_ids, attention_mask=masks, training=False)\n","    v_loss = loss_object(labels, predictions)\n","\n","    validation_loss(v_loss)\n","    for i, auc in enumerate(validation_auc_metrics):\n","        auc.update_state(labels[:,i], predictions[:,i])\n","                                              \n","def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n","    for epoch in range(epochs):\n","        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n","\n","        start = time.time()\n","\n","        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n","            train_step(model, token_ids, masks, labels)\n","            if i % 1000 == 0:\n","                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n","                for i, label_name in enumerate(label_cols):\n","                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n","                    train_auc_metrics[i].reset_states()\n","        \n","        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n","            validation_step(model, token_ids, masks, labels)\n","\n","        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n","\n","        for i, label_name in enumerate(label_cols):\n","            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n","            validation_auc_metrics[i].reset_states()\n","\n","        print('\\n')\n","\n","        \n","train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Run predictions on test-set & unseen text"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T07:04:51.478247Z","iopub.status.busy":"2024-05-06T07:04:51.477911Z","iopub.status.idle":"2024-05-06T07:12:55.095225Z","shell.execute_reply":"2024-05-06T07:12:55.094372Z","shell.execute_reply.started":"2024-05-06T07:04:51.478191Z"},"id":"cdShHpPfH3es","outputId":"94b89833-244c-4cde-e6f6-637594ddef4b","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c16af05a5a834b5e8e87d3d796db8331","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=153164), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\n","test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","test_attention_masks = create_attention_masks(test_input_ids)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T07:14:11.392834Z","iopub.status.busy":"2024-05-06T07:14:11.392526Z","iopub.status.idle":"2024-05-06T07:35:10.853421Z","shell.execute_reply":"2024-05-06T07:35:10.852636Z","shell.execute_reply.started":"2024-05-06T07:14:11.392790Z"},"id":"RUAKdp2EKYlZ","outputId":"29f3b4bc-2c0c-43cb-f873-1b0db86865d3","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a253dde56544eb8a63bae8926d0ee36","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=4786), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["TEST_BATCH_SIZE = 32\n","test_steps = len(df_test) // TEST_BATCH_SIZE\n","\n","test_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n","\n","df_submission = pd.read_csv(subm_path, index_col='id')\n","\n","for i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n","    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n","    predictions = model(token_ids, attention_mask=masks).numpy()\n","\n","    df_submission.loc[sample_ids, label_cols] = predictions"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T07:35:33.730709Z","iopub.status.busy":"2024-05-06T07:35:33.730418Z","iopub.status.idle":"2024-05-06T07:35:33.737541Z","shell.execute_reply":"2024-05-06T07:35:33.736694Z","shell.execute_reply.started":"2024-05-06T07:35:33.730666Z"},"trusted":true},"outputs":[],"source":["def predict_new_data(model, tokenizer, new_data):\n","    new_input_ids = tokenize_sentences([new_data], tokenizer, MAX_LEN)\n","    new_input_ids = pad_sequences(new_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","    new_attention_masks = create_attention_masks(new_input_ids)\n","    \n","    predictions = model(new_input_ids, attention_mask=new_attention_masks).numpy()\n","    flags = [1 if pred > 0.5 else 0 for pred in predictions[0]]\n","    return dict(zip(label_cols, flags))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T07:36:29.496624Z","iopub.status.busy":"2024-05-06T07:36:29.496334Z","iopub.status.idle":"2024-05-06T07:36:29.569381Z","shell.execute_reply":"2024-05-06T07:36:29.568566Z","shell.execute_reply.started":"2024-05-06T07:36:29.496581Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b1eee762c394fa5acccf932f9044610","version_major":2,"version_minor":0},"text/plain":["HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","{'toxic': 1, 'severe_toxic': 0, 'obscene': 1, 'threat': 1, 'insult': 1, 'identity_hate': 1}\n"]}],"source":["new_data = \"you are a nigga, go to hell and I'm gonna hurt you\"\n","predictions = predict_new_data(model, tokenizer, new_data)\n","print(predictions)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T07:36:44.223838Z","iopub.status.busy":"2024-05-06T07:36:44.223547Z","iopub.status.idle":"2024-05-06T07:36:44.228256Z","shell.execute_reply":"2024-05-06T07:36:44.227269Z","shell.execute_reply.started":"2024-05-06T07:36:44.223795Z"},"id":"JgAv8do3PYYX","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok bye\n"]}],"source":["print(\"ok bye\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":44219,"sourceId":8076,"sourceType":"competition"}],"dockerImageVersionId":29845,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}
